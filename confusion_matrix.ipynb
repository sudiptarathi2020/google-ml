{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d37b1d9-e3ae-4d02-8a58-2e182afafc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([np.float64(0.8666666666666667),\n",
       "  np.float64(0.8235294117647058),\n",
       "  np.float64(0.5813953488372093),\n",
       "  np.float64(0.7407407407407407)],\n",
       " [np.float64(0.8125),\n",
       "  np.float64(0.875),\n",
       "  np.float64(0.5681818181818182),\n",
       "  np.float64(0.7843137254901961)],\n",
       " [np.float64(0.8387096774193549),\n",
       "  np.float64(0.8484848484848485),\n",
       "  np.float64(0.5747126436781609),\n",
       "  np.float64(0.7619047619047618)],\n",
       " np.float64(0.7530830420023307),\n",
       " np.float64(0.7599988859180036),\n",
       " np.float64(0.7559529828717815),\n",
       " np.float64(0.7593013943176136),\n",
       " np.float64(0.7591623036649214),\n",
       " np.float64(0.7591623036649214))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Confusion matrix (rows: predicted, columns: actual)\n",
    "cm = np.array([\n",
    "    [52, 3, 7, 2],\n",
    "    [2, 28, 2, 0],\n",
    "    [5, 2, 25, 12],\n",
    "    [1, 1, 9, 40]\n",
    "])\n",
    "\n",
    "# Number of samples per class\n",
    "samples = np.array([60, 34, 43, 54])\n",
    "\n",
    "# Initialize lists to store metrics per class\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each class\n",
    "for i in range(len(samples)):\n",
    "    tp = cm[i, i]\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    fn = cm[i, :].sum() - tp\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "# Macro averages (unweighted mean)\n",
    "macro_precision = np.mean(precision_list)\n",
    "macro_recall = np.mean(recall_list)\n",
    "macro_f1 = np.mean(f1_list)\n",
    "\n",
    "# Weighted F1 score (weighted by the number of samples per class)\n",
    "weighted_f1 = np.sum(f1_list * samples) / np.sum(samples)\n",
    "\n",
    "# Net precision and net recall (micro average)\n",
    "total_tp = np.trace(cm)\n",
    "total_pred = cm.sum(axis=0).sum()\n",
    "total_actual = cm.sum(axis=1).sum()\n",
    "\n",
    "net_precision = total_tp / total_pred if total_pred > 0 else 0\n",
    "net_recall = total_tp / total_actual if total_actual > 0 else 0\n",
    "\n",
    "precision_list, recall_list, f1_list, macro_precision, macro_recall, macro_f1, weighted_f1, net_precision, net_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31fea70d-1460-4e43-b135-a02b3fdc5821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Class Metrics:\n",
      "Class Precision   Recall    F1 Score  \n",
      "1     0.867       0.812     0.839     \n",
      "2     0.824       0.875     0.848     \n",
      "3     0.581       0.568     0.575     \n",
      "4     0.741       0.784     0.762     \n",
      "\n",
      "Aggregate Metrics:\n",
      "Macro Precision : 0.753\n",
      "Macro Recall    : 0.760\n",
      "Macro F1 Score  : 0.756\n",
      "Net Precision   : 0.759 (Micro Average)\n",
      "Net Recall      : 0.759 (Micro Average)\n",
      "Weighted F1 Score: 0.759\n"
     ]
    }
   ],
   "source": [
    "# Print the results in a readable format\n",
    "print(\"Per-Class Metrics:\")\n",
    "print(f\"{'Class':<6}{'Precision':<12}{'Recall':<10}{'F1 Score':<10}\")\n",
    "for i, (p, r, f1) in enumerate(zip(precision_list, recall_list, f1_list), 1):\n",
    "    print(f\"{i:<6}{p:<12.3f}{r:<10.3f}{f1:<10.3f}\")\n",
    "\n",
    "print(\"\\nAggregate Metrics:\")\n",
    "print(f\"Macro Precision : {macro_precision:.3f}\")\n",
    "print(f\"Macro Recall    : {macro_recall:.3f}\")\n",
    "print(f\"Macro F1 Score  : {macro_f1:.3f}\")\n",
    "print(f\"Net Precision   : {net_precision:.3f} (Micro Average)\")\n",
    "print(f\"Net Recall      : {net_recall:.3f} (Micro Average)\")\n",
    "print(f\"Weighted F1 Score: {weighted_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1219169-7050-40d6-9b9e-ac6d05bb38ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
